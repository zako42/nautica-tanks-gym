{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gym Interface with an ML-Agents app\n",
    "\n",
    "We will use our earlier Tanks project. I'm including a built Tanks executable. You can build your own within the Unity editor by going to File -> Build Settings. Will go through the steps quickly in the lecture.\n",
    "\n",
    "Once we have an executable, we can interface with it using the mlagents gym wrapper. This exposes the Tanks app data using the standard gym interface, and we can use it like we do with other gym environments like cartpole, etc.\n",
    "\n",
    "To get started, you will need to have installed the python mlagents package, and the gym_unity package using pip and your virtual environment.\n",
    "\n",
    "See here for the official documentation:\n",
    "https://github.com/Unity-Technologies/ml-agents/tree/main/gym-unity\n",
    "\n",
    "NOTE: The gym and python interface for mlagents has changed a bit recently. Depending on the version you have installed, the code may differ! Below I am using mlagents 0.25 (which should install mlagents-envs 0.25), and gym-unity 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from gym_unity.envs import UnityToGymWrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Agents has a low level Python interface we'll look at later. We first use this to open the Tanks environment, then we use the `UnityToGymWrapper` on it. When you run the code below, you should see the environment open as a tiny window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 09:32:44 INFO [environment.py:113] Connected to Unity environment with package version 1.0.5 and communication version 1.0.0\n",
      "2021-03-26 09:32:44 INFO [environment.py:282] Connected new brain:\n",
      "VisualTank1?team=0\n",
      "2021-03-26 09:32:44 WARNING [__init__.py:92] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.\n",
      "D:\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# specify the path to the executable here (relative to where python or jupyter is running)\n",
    "filename = \"./visual-test-0/tanks-ml-tutorial.exe\"  \n",
    "\n",
    "# NOTE: this is the new sytax as of version 0.25 (earlier syntax was slightly different)\n",
    "unity_env = UnityEnvironment(filename)\n",
    "gym_env = UnityToGymWrapper(unity_env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can use tanks_env like any other gym environment. You should have seen some log output with some info and maybe warnings. We're just showing the process, so ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env.reset()\n",
    "for _ in range(100):\n",
    "    gym_env.render()\n",
    "    gym_env.step(gym_env.action_space.sample())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the tiny unity window that opened earlier, you should see the agent performing random actions. In the code above, we're running 100 steps, rendering the frames, and sampling random actions from the agent's action space.\n",
    "\n",
    "This was just a test. Instead of running random actions, you would add your algorithm code here like any other gym environment. You can use algorithms from OpenAI baselines, Google Dopamine, or any DRL stuff in Python here. This also means you could use PyTorch, Tensorflow or anything you want, and use it. There are DRL frameworks you can experiment with. Since Gym is a common interface supported by DRL libraries, you can experiment quite a bit -- since ALL Unity ML Agents builds are compatible with Gym, ANY environment you build with it can be used for experimentation by other researchers who may not be familiar with Unity. They only need to know Python and the common Gym interface!\n",
    "\n",
    "Remember earlier we were calling `mlagents-learn` and setting hyperparameters in a `config.yaml` file. The algorithm was a black box and we were defaulting to PPO. The PPO algorithm that mlagents uses is in Python -- if you look in the mlagents repo we downloaded earlier, there are some nested subfolders in the repo at `ml-agents/ml-agents/mlagents/trainers` which have the PPO code, along with the other Python code that was being run for us when we call the `mlagents-learn` command. Using gym, we wouldn't be forced to use their code, we could write our own or use open source stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "Now we're going to take a look at the ML Agents low level Python API a bit. It is conceptually similar to the gym interface, with some differences in syntax.\n",
    "\n",
    "One thing to note about the mlagents gym wrapper, there are some limitations. See the documentation page for details. But one big limitation is we can only use it for training a single agent at a time. If you want to do MARL (multi-agent reinforcement learning) algorithms like DDPG, etc, you can't. Another relevant limitation is stacked observations are not currently supported. In Tanks, we used stacked observations, so this would not work (we would need to go back into Unity, change the environment to remove it, and re-build the executable).\n",
    "\n",
    "If we use the ML Agents low level Python API, we don't have those limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisualTank1?team=0\n"
     ]
    }
   ],
   "source": [
    "# we have already imported everything earlier\n",
    "# we were using the gym wrapper on top of the environment we created with UnityEnvironment()\n",
    "# now, we'll just use it directly\n",
    "\n",
    "unity_env.reset()\n",
    "# this is the same as in gym\n",
    "\n",
    "behavior_name = list(unity_env.behavior_specs.keys())[0]\n",
    "\n",
    "print(behavior_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each behavior_spec represents one of the \"Behavior Parameters\" in Unity (whatever you named it).\n",
    "\n",
    "Only the ones which are set for training will show up here, with the name given in their Behavior Parameters. Above, the behavior parameters were named \"VisualTank\". If different teams are set (for multiagent), the team number will be part of the behavior_spec name (the \"?team=0\" appended above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations list:  3\n",
      "[ObservationSpec(shape=(84, 84, 3), dimension_property=(<DimensionProperty.UNSPECIFIED: 0>, <DimensionProperty.UNSPECIFIED: 0>, <DimensionProperty.UNSPECIFIED: 0>), observation_type=<ObservationType.DEFAULT: 0>, name=''), ObservationSpec(shape=(84, 84, 3), dimension_property=(<DimensionProperty.UNSPECIFIED: 0>, <DimensionProperty.UNSPECIFIED: 0>, <DimensionProperty.UNSPECIFIED: 0>), observation_type=<ObservationType.DEFAULT: 0>, name=''), ObservationSpec(shape=(12,), dimension_property=(<DimensionProperty.UNSPECIFIED: 0>,), observation_type=<ObservationType.DEFAULT: 0>, name='')] \n",
      "\n",
      "actions list:  2\n",
      "Continuous: 0, Discrete: (3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "bs = unity_env.behavior_specs[behavior_name]\n",
    "# get the behavior spec for VisualTank\n",
    "\n",
    "obs_spec, act_spec = bs\n",
    "# get the observation info and action info and print it out to look at\n",
    "print(\"observations list: \", len(obs_spec))\n",
    "print(obs_spec, \"\\n\")\n",
    "print(\"actions list: \", len(act_spec))\n",
    "print(act_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the visual agent, which has 2 visual observations (pixel inputs) of 84x84. We can see the shape is 84, 84, 3. The 3 is because we have 3 color channels (R,G,B).\n",
    "\n",
    "You may remember the CameraSensor and RenderTextureSensors in Unity were \"automatic\", in that you didn't need to set the number of observations in the Behavior Parameters. ML Agents kept these sensor observations separate from our own. If you drill down and look in the list of observation specs above, there are two with (84,84,3) (the camera sensor and render texture sensor) and another with size 12. The size 12 observations are the vector observations I used (6 observations, stacked by 2 = 12).\n",
    "\n",
    "For the actions, we see 0 Continuous actions. We also see Discrete actions (3,3,3). If you remember, we had 3 actions, each with 3 choices.\n",
    "\n",
    "(nothing, forward, back),\n",
    "(nothing, left, right),\n",
    "(nothing, fire weak, fire strong).\n",
    "\n",
    "They have both continuous and discrete actions both listed, because I believe mlagents is going to be adding the ability for agents to have both types of actions together (vice choosing only one type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObservationSpec(shape=(84, 84, 3), dimension_property=(<DimensionProperty.UNSPECIFIED: 0>, <DimensionProperty.UNSPECIFIED: 0>, <DimensionProperty.UNSPECIFIED: 0>), observation_type=<ObservationType.DEFAULT: 0>, name='') \n",
      "\n",
      "ObservationSpec(shape=(84, 84, 3), dimension_property=(<DimensionProperty.UNSPECIFIED: 0>, <DimensionProperty.UNSPECIFIED: 0>, <DimensionProperty.UNSPECIFIED: 0>), observation_type=<ObservationType.DEFAULT: 0>, name='')\n",
      "ObservationSpec(shape=(12,), dimension_property=(<DimensionProperty.UNSPECIFIED: 0>,), observation_type=<ObservationType.DEFAULT: 0>, name='')\n"
     ]
    }
   ],
   "source": [
    "print(obs_spec[0], \"\\n\")\n",
    "print(obs_spec[1], \"\\n\")\n",
    "print(obs_spec[2], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we print out each observation group so you can take a better look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, ts = unity_env.get_steps(behavior_name)\n",
    "# get data returned from current step\n",
    "# return value is a tuple of (DecisionSteps, TerminalSteps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionSteps contains data for all agents which are waiting for an action. TerminalSteps contains data for all agents which are on their last step of their episode. If you are handling multi-agent, you would need to be tracking each agent's data and episodes. Since they may end episodes at different steps, you would need to manage this when writing your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[[0.64705884, 0.4627451 , 0.34509805],\n",
      "        [0.62352943, 0.44705883, 0.3372549 ],\n",
      "        [0.627451  , 0.44705883, 0.3372549 ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.627451  , 0.4509804 , 0.3372549 ],\n",
      "        [0.6392157 , 0.45882353, 0.34509805],\n",
      "        [0.6431373 , 0.45882353, 0.34509805],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.64705884, 0.4627451 , 0.34509805],\n",
      "        [0.6509804 , 0.46666667, 0.34901962],\n",
      "        [0.6784314 , 0.48235294, 0.35686275],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]]], dtype=float32), array([[[1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]], dtype=float32), array([0.5199055 , 0.4879478 , 0.01434668, 1.        , 1.        ,\n",
      "       1.        , 0.5199055 , 0.4879478 , 0.01434668, 1.        ,\n",
      "       1.        , 1.        ], dtype=float32)], -0.0011, 0, [array([False, False, False]), array([False, False, False]), array([False,  True,  True])], 0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(list(ds[0]))\n",
    "# print out one agent's decision step data\n",
    "# the step data includes our agent's data, including\n",
    "# observations, reward, agent_id, action masks,\n",
    "# and 2 others i'm not sure about right now\n",
    "# (sorry, they didnt exist in earlier version, will need to investigate)\n",
    "\n",
    "obs, reward, agent_id, masks, something, something_else = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0.64705884, 0.4627451 , 0.34509805],\n",
      "        [0.62352943, 0.44705883, 0.3372549 ],\n",
      "        [0.627451  , 0.44705883, 0.3372549 ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.627451  , 0.4509804 , 0.3372549 ],\n",
      "        [0.6392157 , 0.45882353, 0.34509805],\n",
      "        [0.6431373 , 0.45882353, 0.34509805],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.64705884, 0.4627451 , 0.34509805],\n",
      "        [0.6509804 , 0.46666667, 0.34901962],\n",
      "        [0.6784314 , 0.48235294, 0.35686275],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]]], dtype=float32), array([[[1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]], dtype=float32), array([0.5199055 , 0.4879478 , 0.01434668, 1.        , 1.        ,\n",
      "       1.        , 0.5199055 , 0.4879478 , 0.01434668, 1.        ,\n",
      "       1.        , 1.        ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(obs)\n",
    "# the observation data, includes pixel data for 2 images 84x84x3, and 12 vector observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0011\n"
     ]
    }
   ],
   "source": [
    "print(reward)\n",
    "# reward for current step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(agent_id)\n",
    "# when dealing with multi-agent, we can track agents by ID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([False, False, False]), array([False, False, False]), array([False,  True,  True])]\n"
     ]
    }
   ],
   "source": [
    "print(masks)\n",
    "# If you recall, for discrete actions, we can mask actions.\n",
    "# True means the action is masked (cannot be performed on this step for whatever reason)\n",
    "# In Tanks, if the agent cannon is not ready, it masks the fire actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-8e0430cef0d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# actions in Tanks are 3 actions, so we specify 2 arrays of 3 values above.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0munity_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbehavior_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\mlagents_envs\\environment.py\u001b[0m in \u001b[0;36mset_actions\u001b[1;34m(self, behavior_name, action)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0maction_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env_specs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbehavior_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[0mnum_agents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbehavior_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbehavior_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbehavior_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\mlagents_envs\\base_env.py\u001b[0m in \u001b[0;36m_validate_action\u001b[1;34m(self, actions, n_agents, name)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontinuous_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         )\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontinuous\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0m_expected_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m             raise UnityActionException(\n\u001b[0;32m    426\u001b[0m                 \u001b[1;34mf\"The behavior {name} needs a continuous input of dimension \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'continuous'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "agent_actions = np.array(((0.),(1,0,0)))\n",
    "# manually set up an action to perform\n",
    "# we need to use numpy arrays\n",
    "# note that there are 2 agents in this environment, so we need to provide actions for both\n",
    "# actions in Tanks are 3 actions, so we specify 2 arrays of 3 values above.\n",
    "\n",
    "unity_env.set_actions(behavior_name, agent_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_env.step()\n",
    "# slightly different from gym interface here\n",
    "# we set the data using env.set_actions() then call step()\n",
    "# but concept is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mlagents low level Python API, you can do similar things you would with gym. The API is more involved, but there are also less limitations.\n",
    "\n",
    "Warning - the Python API has changed a few times, and it looks like they changed it between when I first started writing this, and now (3/26). So be aware that when you pip install, it grabs the current version. When the API changes, they mention the changes in their github repo changelog.\n",
    "\n",
    "The set_actions code above changed, and I didn't have time to investigate the new syntax, so the previous cell doesn't work yet. Will update when I have more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 13:19:42 INFO [environment.py:429] Environment shut down with return code 0 (CTRL_C_EVENT).\n"
     ]
    }
   ],
   "source": [
    "unity_env.close()  # make sure you close the env when done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
